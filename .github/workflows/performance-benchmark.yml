name: ⚡ Performance Benchmarking

on:
  schedule:
    # Run weekly on Sundays at 2 AM UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - pathfinding
          - mapgen
          - ecs
      baseline_comparison:
        description: 'Compare against baseline'
        required: false
        default: true
        type: boolean

permissions:
  contents: read
  issues: write
  pull-requests: write

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  performance-baseline:
    name: 📊 Performance Baseline
    runs-on: ubuntu-latest
    outputs:
      benchmark-results: ${{ steps.benchmark.outputs.results }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Cache Rust dependencies
        uses: Swatinem/rust-cache@v2
        with:
          cache-on-failure: true
          shared-key: "rust-deps"

      - name: Install benchmarking tools
        run: |
          echo "📦 Installing benchmarking tools..."
          cargo install cargo-criterion --quiet
          cargo install hyperfine --quiet || echo "hyperfine not available"

      - name: Setup benchmark environment
        run: |
          echo "🔧 Setting up benchmark environment..."
          # Create benchmark results directory
          mkdir -p benchmark-results

          # Build release version for accurate benchmarks
          cargo build --release --quiet

      - name: Run pathfinding benchmarks
        if: github.event.inputs.benchmark_type == 'all' || github.event.inputs.benchmark_type == 'pathfinding'
        run: |
          echo "🎯 Running pathfinding benchmarks..."
          cargo bench --package gc_core -- pathfinding --verbose || echo "No pathfinding benchmarks found"

      - name: Run map generation benchmarks
        if: github.event.inputs.benchmark_type == 'all' || github.event.inputs.benchmark_type == 'mapgen'
        run: |
          echo "🗺️ Running map generation benchmarks..."
          cargo bench --package gc_core -- mapgen --verbose || echo "No mapgen benchmarks found"

      - name: Run ECS performance benchmarks
        if: github.event.inputs.benchmark_type == 'all' || github.event.inputs.benchmark_type == 'ecs'
        run: |
          echo "🔄 Running ECS performance benchmarks..."
          cargo bench --package gc_core -- ecs --verbose || echo "No ECS benchmarks found"

      - name: Run CLI performance tests
        run: |
          echo "⚡ Running CLI performance tests..."

          # Test map generation performance
          echo "Testing map generation performance..."
          hyperfine --warmup 3 --runs 10 \
            'cargo run --release -p gc_cli -- --width 50 --height 40 mapgen' \
            --export-json benchmark-results/mapgen-performance.json || \
            echo "hyperfine not available, using basic timing"

          # Test pathfinding performance
          echo "Testing pathfinding performance..."
          hyperfine --warmup 3 --runs 10 \
            'cargo run --release -p gc_cli -- --width 40 --height 30 path' \
            --export-json benchmark-results/pathfinding-performance.json || \
            echo "hyperfine not available, using basic timing"

      - name: Generate benchmark report
        id: benchmark
        run: |
          echo "📊 Generating benchmark report..."

          # Create benchmark summary
          cat > benchmark-results/benchmark-summary.md << 'EOF'
          # Performance Benchmark Results

          **Generated:** $(date)
          **Commit:** $(git rev-parse HEAD)
          **Branch:** $(git branch --show-current)

          ## Benchmark Results

          ### CLI Performance Tests
          EOF

          # Add CLI performance results if available
          if [ -f benchmark-results/mapgen-performance.json ]; then
            echo "#### Map Generation Performance" >> benchmark-results/benchmark-summary.md
            echo "\`\`\`json" >> benchmark-results/benchmark-summary.md
            cat benchmark-results/mapgen-performance.json >> benchmark-results/benchmark-summary.md
            echo "\`\`\`" >> benchmark-results/benchmark-summary.md
          fi

          if [ -f benchmark-results/pathfinding-performance.json ]; then
            echo "#### Pathfinding Performance" >> benchmark-results/benchmark-summary.md
            echo "\`\`\`json" >> benchmark-results/benchmark-summary.md
            cat benchmark-results/pathfinding-performance.json >> benchmark-results/benchmark-summary.md
            echo "\`\`\`" >> benchmark-results/benchmark-summary.md
          fi

          # Add criterion benchmark results if available
          if [ -d target/criterion ]; then
            echo "### Criterion Benchmarks" >> benchmark-results/benchmark-summary.md
            find target/criterion -name "*.svg" -o -name "*.json" | head -10 >> benchmark-results/benchmark-summary.md
          fi

          echo "" >> benchmark-results/benchmark-summary.md
          echo "**Note:** Performance benchmarks help track regressions and improvements over time." >> benchmark-results/benchmark-summary.md

          # Set output for other jobs
          echo "results<<EOF" >> $GITHUB_OUTPUT
          cat benchmark-results/benchmark-summary.md >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: performance-benchmarks-${{ github.sha }}
          path: |
            benchmark-results/
            target/criterion/
          retention-days: 90

  performance-comparison:
    name: 📈 Performance Comparison
    runs-on: ubuntu-latest
    needs: performance-baseline
    if: github.event.inputs.baseline_comparison == 'true' || github.event_name == 'schedule'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Download previous benchmark results
        uses: actions/download-artifact@v4
        with:
          name: performance-benchmarks-main
          path: previous-benchmarks/
        continue-on-error: true

      - name: Compare performance results
        run: |
          echo "📈 Comparing performance results..."

          if [ -d previous-benchmarks ]; then
            echo "Previous benchmark results found. Comparing..."

            # Simple comparison logic (can be enhanced)
            echo "## Performance Comparison Report" > performance-comparison.md
            echo "" >> performance-comparison.md
            echo "**Current vs Previous Benchmarks**" >> performance-comparison.md
            echo "" >> performance-comparison.md

            # Add comparison logic here
            echo "Comparison analysis would go here..." >> performance-comparison.md

          else
            echo "No previous benchmark results found. This is the baseline run."
          fi

      - name: Upload comparison report
        uses: actions/upload-artifact@v4
        with:
          name: performance-comparison-${{ github.sha }}
          path: performance-comparison.md
          retention-days: 90

  performance-alert:
    name: 🚨 Performance Alert
    runs-on: ubuntu-latest
    needs: [performance-baseline, performance-comparison]
    if: |
      always() &&
      (needs.performance-baseline.result == 'failure' ||
       needs.performance-comparison.result == 'failure')

    steps:
      - name: Create performance alert
        run: |
          echo "🚨 Performance regression detected!" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Performance Issues Found" >> $GITHUB_STEP_SUMMARY
          echo "- Benchmark execution failed or detected significant regression" >> $GITHUB_STEP_SUMMARY
          echo "- Please review the benchmark artifacts for details" >> $GITHUB_STEP_SUMMARY
          echo "- Consider investigating recent changes for performance impact" >> $GITHUB_STEP_SUMMARY

      - name: Create GitHub issue on significant regression
        if: github.event_name == 'schedule'
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: '🚨 Performance Regression Detected',
              body: `
              ## Performance Regression Alert

              A performance regression has been detected in the automated benchmarking.

              **Details:**
              - Detected on: ${new Date().toISOString()}
              - Commit: ${context.sha}
              - Workflow: ${context.workflow}

              **Next Steps:**
              1. Review the benchmark artifacts
              2. Investigate recent changes for performance impact
              3. Consider optimizing affected code paths
              4. Update performance baselines if changes are expected

              **Artifacts:** Check the workflow run for detailed benchmark results.
              `,
              labels: ['performance', 'regression', 'investigation-needed']
            })

  performance-summary:
    name: 📊 Performance Summary
    runs-on: ubuntu-latest
    needs: [performance-baseline, performance-comparison]
    if: always()

    steps:
      - name: Generate performance summary
        run: |
          echo "## ⚡ Performance Benchmark Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          baseline_result="${{ needs.performance-baseline.result }}"
          comparison_result="${{ needs.performance-comparison.result }}"

          echo "| Stage | Status | Description |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|-------------|" >> $GITHUB_STEP_SUMMARY

          [[ "$baseline_result" == "success" ]] && echo "| 📊 Baseline | ✅ Completed | Performance benchmarks run |" >> $GITHUB_STEP_SUMMARY || echo "| 📊 Baseline | ❌ Failed | Benchmark execution failed |" >> $GITHUB_STEP_SUMMARY
          [[ "$comparison_result" == "success" ]] && echo "| 📈 Comparison | ✅ Completed | Results compared |" >> $GITHUB_STEP_SUMMARY || echo "| 📈 Comparison | ⚠️ Skipped | No baseline available |" >> $GITHUB_STEP_SUMMARY

          echo "" >> $GITHUB_STEP_SUMMARY

          if [[ "$baseline_result" == "success" ]]; then
            echo "✅ **Performance benchmarks completed successfully!**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### 📈 Benchmark Results" >> $GITHUB_STEP_SUMMARY
            echo "- Performance metrics collected and stored" >> $GITHUB_STEP_SUMMARY
            echo "- Results available in workflow artifacts" >> $GITHUB_STEP_SUMMARY
            echo "- Baseline established for future comparisons" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Performance benchmarking encountered issues**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Please review the benchmark execution logs for details." >> $GITHUB_STEP_SUMMARY
          fi

      - name: Store benchmark results for future comparison
        if: needs.performance-baseline.result == 'success'
        uses: actions/upload-artifact@v4
        with:
          name: performance-benchmarks-main
          path: benchmark-results/
          retention-days: 365
